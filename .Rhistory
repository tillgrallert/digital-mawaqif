aes(x = date, y = 1, colour = tag),
pch=3)
v.Plot.Wheat.Box.Price.Trends
v.Prices.Trends <- read.csv("csv/qualitative-prices.csv", header=TRUE, sep = ",", quote = "\"")
v.Prices.Trends$date <- anydate(v.Prices.Trends$date)
v.Prices.Trends.Period <- funcPeriod(v.Prices.Trends,v.Date.Start,v.Date.Stop)
View(v.Prices.Trends.Period)
v.Plot.Wheat.Box.Price.Trends <- v.Plot.Base +
# add labels
labs(title="Wheat prices and food riots in Bilad al-Sham",
subtitle="minimum prices aggregated by year",
y="Price (piaster/kile)") + # provides title, subtitle, x, y, caption
# layer: box plot prices, average of min and max prices
#geom_boxplot(data = vWheatKilePeriod,aes(x=year,group=year,y=(quantity.2 + quantity.3) / 2), na.rm = T)+
# layer: box plot min prices
geom_boxplot(data = v.Prices.Wheat.Period,aes(x=year,group=year,y=quantity.2), na.rm = T)+
## add error bars
stat_boxplot(geom ='errorbar')+
# layer: box plot max prices
geom_boxplot(data = v.Prices.Wheat.Period,aes(x=year, group=year,y=quantity.3), na.rm = T, color="blue", width=100)+
# despite some visiual overlap, count charts are not the best solution to the data set
# because only very few reports fall on the same day (and thus formally) overlap
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: high"),
aes(x = date, y = 9, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: rising"),
aes(x = date, y = 7, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: normal"),
aes(x = date, y = 5, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: falling"),
aes(x = date, y = 3, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: low"),
aes(x = date, y = 1, colour = tag),
pch=3)
v.Plot.Wheat.Box.Price.Trends
## box plot plus price trends, re-rest the period
# specify period
v.Date.Start <- as.Date("1900-01-01")
v.Date.Stop <- as.Date("1916-12-31")
v.Prices.Wheat.Period <- funcPeriod(v.Prices.Wheat,v.Date.Start,v.Date.Stop)
v.Prices.Wheat.Daily.Period <- funcPeriod(v.Prices.Wheat.Summary.Daily,v.Date.Start,v.Date.Stop)
v.Prices.Trends.Period <- funcPeriod(v.Prices.Trends,v.Date.Start,v.Date.Stop)
v.FoodRiots.Period <- funcPeriod(v.FoodRiots,v.Date.Start,v.Date.Stop)
v.Plot.Wheat.Box.Price.Trends <- v.Plot.Base +
# add labels
labs(title="Wheat prices and food riots in Bilad al-Sham",
subtitle="minimum prices aggregated by year",
y="Price (piaster/kile)") + # provides title, subtitle, x, y, caption
# layer: box plot prices, average of min and max prices
#geom_boxplot(data = vWheatKilePeriod,aes(x=year,group=year,y=(quantity.2 + quantity.3) / 2), na.rm = T)+
# layer: box plot min prices
geom_boxplot(data = v.Prices.Wheat.Period,aes(x=year,group=year,y=quantity.2), na.rm = T)+
## add error bars
stat_boxplot(geom ='errorbar')+
# layer: box plot max prices
geom_boxplot(data = v.Prices.Wheat.Period,aes(x=year, group=year,y=quantity.3), na.rm = T, color="blue", width=100)+
# despite some visiual overlap, count charts are not the best solution to the data set
# because only very few reports fall on the same day (and thus formally) overlap
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: high"),
aes(x = date, y = 9, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: rising"),
aes(x = date, y = 7, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: normal"),
aes(x = date, y = 5, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: falling"),
aes(x = date, y = 3, colour = tag),
pch=3)+
# layer: falling prices
geom_point(data = filter(v.Prices.Trends.Period, tag=="prices: low"),
aes(x = date, y = 1, colour = tag),
pch=3)
v.Plot.Wheat.Box.Price.Trends
install.packages("stylo", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
load("/Volumes/Dessau HD/BachUni/BachBibliothek/GitHub/OpenArabicPE/analytical-playground/metadata/.RData")
load("/Volumes/Dessau HD/BachUni/BachBibliothek/GitHub/OpenArabicPE/analytical-playground/metadata/.RData")
library(lubridate) # for working with dates
library(ggplot2)  # for creating graphs
library(scales)   # to access breaks/formatting functions
library(gridExtra) # for arranging plots
library(plotly) # interactive plots based on ggplot
library(dplyr) # for data cleaning
library(plyr)
theme_set(theme_bw())
# function to create subsets for periods
funcPeriod <- function(f,x,y){f[f$date >= x & f$date <= y,]}
# use a working directory
setwd("/Volumes/Dessau HD/BachUni/BachBibliothek/GitHub/OpenArabicPE/analytical-playground/metadata")
# 1. read data from csv
## vStatsTei contains one row for every issue of Muqtabas
vStatsTeiRaw <- read.csv("statistics/oclc_4770057679-stats_tei.csv", header=TRUE, sep = ";", quote = "")
vStatsMods <- read.csv("statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ";", quote = "")
## convert date to Date class
vStatsTeiRaw $date <- anydate(vStatsTeiRaw$date)
## add a new column combining volumes and issues
vStatsTeiRaw <- within(vStatsTeiRaw, volume.issue <- paste(volume,issue,sep='-'))
## filter out all rows for issues without marked up page breaks
vStatsTei <- filter(vStatsTeiRaw, number.of.pages >=10)
## add new column of words per page
vStatsTei$words.per.page <- vStatsTei$number.of.words / vStatsTei$number.of.pages
# 2. descriptive stats
## word counts
sum(vStatsTeiRaw[,c("number.of.words")])
mean(vStatsTeiRaw[,c("number.of.words")])
median(vStatsTeiRaw[,c("number.of.words")])
sd(vStatsTeiRaw[,c("number.of.words")])
## page counts
vStatsTeiPages <- vStatsTei[,c("date","volume.issue","number.of.pages")]
mean(vStatsTei[,c("number.of.pages")])
median(vStatsTei[,c("number.of.pages")])
vPlotWords <- ggplot()+
labs(title="al-Muqtabas",
subtitle="Number of words per page per issue",
x="Issue",
y="number of words / pages")+ # provides title, subtitle, x, y, caption
# layers: lines for mean and median
#geom_line(data = vStatsTei, aes(x=volume.issue,y=mean(vStatsTei[,c("words.per.page")])),color="green",size=1)+
#geom_line(data = vStatsTei, aes(x=volume.issue,y=median(vStatsTei[,c("words.per.page")])),color="blue",size=1)+
# layer: words per page
geom_bar(data = vStatsTei, aes(x=date,y=words.per.page), stat="identity", fill="red")+
# layer: page numbers
geom_bar(data = vStatsTei, aes(x=date,y=number.of.pages), stat="identity")+
scale_x_date(breaks=date_breaks("6 months"), labels=date_format("%Y %m"))+
theme(axis.text.x = element_text(angle = 45, vjust=0.5),  # rotate x axis text
panel.grid.minor = element_blank())  # turn off minor grid
vPlotWords
vPlotWords <- ggplot()+
labs(title="al-Muqtabas",
subtitle="Number of words per page per issue",
x="Issue",
y="number of words / pages")+ # provides title, subtitle, x, y, caption
# layers: lines for mean and median
#geom_line(data = vStatsTei, aes(x=volume.issue,y=mean(vStatsTei[,c("words.per.page")])),color="green",size=1)+
#geom_line(data = vStatsTei, aes(x=volume.issue,y=median(vStatsTei[,c("words.per.page")])),color="blue",size=1)+
# layer: words per page
geom_bar(data = vStatsTei, aes(x=date,y=words.per.page, colour = words.per.page), stat="identity", fill="red")+
# layer: page numbers
geom_bar(data = vStatsTei, aes(x=date,y=number.of.pages), stat="identity")+
scale_x_date(breaks=date_breaks("6 months"), labels=date_format("%Y %m"))+
theme(axis.text.x = element_text(angle = 45, vjust=0.5),  # rotate x axis text
panel.grid.minor = element_blank())  # turn off minor grid
vPlotWords
vPlotWords <- ggplot()+
labs(title="al-Muqtabas",
subtitle="Number of words per page per issue",
x="Issue",
y="number of words / pages")+ # provides title, subtitle, x, y, caption
# layers: lines for mean and median
#geom_line(data = vStatsTei, aes(x=volume.issue,y=mean(vStatsTei[,c("words.per.page")])),color="green",size=1)+
#geom_line(data = vStatsTei, aes(x=volume.issue,y=median(vStatsTei[,c("words.per.page")])),color="blue",size=1)+
# layer: words per page
geom_bar(data = vStatsTei, aes(x=date,y=words.per.page, colour = words.per.page), stat="identity")+
# layer: page numbers
geom_bar(data = vStatsTei, aes(x=date,y=number.of.pages), stat="identity")+
scale_x_date(breaks=date_breaks("6 months"), labels=date_format("%Y %m"))+
theme(axis.text.x = element_text(angle = 45, vjust=0.5),  # rotate x axis text
panel.grid.minor = element_blank())  # turn off minor grid
vPlotWords
vPlotWords <- ggplot()+
labs(title="al-Muqtabas",
subtitle="Number of words per page per issue",
x="Issue",
y="number of words / pages")+ # provides title, subtitle, x, y, caption
# layers: lines for mean and median
#geom_line(data = vStatsTei, aes(x=volume.issue,y=mean(vStatsTei[,c("words.per.page")])),color="green",size=1)+
#geom_line(data = vStatsTei, aes(x=volume.issue,y=median(vStatsTei[,c("words.per.page")])),color="blue",size=1)+
# layer: words per page
geom_bar(data = vStatsTei, aes(x=date,y=words.per.page, fill = words.per.page), stat="identity")+
# layer: page numbers
geom_bar(data = vStatsTei, aes(x=date,y=number.of.pages), stat="identity")+
scale_x_date(breaks=date_breaks("6 months"), labels=date_format("%Y %m"))+
theme(axis.text.x = element_text(angle = 45, vjust=0.5),  # rotate x axis text
panel.grid.minor = element_blank())  # turn off minor grid
vPlotWords
vPlotWords <- ggplot()+
labs(title="al-Muqtabas",
subtitle="Number of words per page per issue",
x="Issue",
y="number of words / pages")+ # provides title, subtitle, x, y, caption
# layers: lines for mean and median
#geom_line(data = vStatsTei, aes(x=volume.issue,y=mean(vStatsTei[,c("words.per.page")])),color="green",size=1)+
#geom_line(data = vStatsTei, aes(x=volume.issue,y=median(vStatsTei[,c("words.per.page")])),color="blue",size=1)+
# layer: words per page
geom_bar(data = vStatsTei, aes(x=date,y=words.per.page), stat="identity", fill="red")+
# layer: page numbers
geom_bar(data = vStatsTei, aes(x=date,y=number.of.pages), stat="identity")+
scale_x_date(breaks=date_breaks("6 months"), labels=date_format("%Y %m"))+
theme(axis.text.x = element_text(angle = 45, vjust=0.5),  # rotate x axis text
panel.grid.minor = element_blank())  # turn off minor grid
vPlotWords
mean(vStatsTei[,c("number.of.pages")])
median(vStatsTei[,c("number.of.pages")])
# 2. descriptive stats
## word counts
sum(vStatsTeiRaw[,c("number.of.words")])
View(vStatsMods)
View(vStatsTeiRaw)
# 1. read data from csv
## vStatsTei contains one row for every issue of Muqtabas
vStatsTeiRaw <- read.csv("statistics/oclc_4770057679-stats_tei.csv", header=TRUE, sep = ",", quote = "")
# 1. read data from csv
## vStatsTei contains one row for every issue of Muqtabas
vStatsTeiRaw <- read.csv("statistics/oclc_4770057679-stats_tei.csv", header=TRUE, sep = ";", quote = "")
vStatsMods <- read.csv("statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# function to create subsets for periods
funcPeriod <- function(f,x,y){f[f$date >= x & f$date <= y,]}
# use a working directory
setwd("/Volumes/Dessau HD/BachUni/BachBibliothek/GitHub/OpenArabicPE/analytical-playground/metadata")
# 1. read data from csv
## vStatsTei contains one row for every issue of Muqtabas
vStatsTeiRaw <- read.csv("statistics/oclc_4770057679-stats_tei.csv", header=TRUE, sep = ";", quote = "")
vStatsMods <- read.csv("statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ";", quote = "")
vStatsMods <- read.csv("statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
vLocsBylines <- read.csv("statistics/muqtabas_bylines-toponyms.csv", header=TRUE, sep = ",", quote = "")
vLocsBylinesJson <- geojson_read("statistics/muqtabas_bylines-toponyms.geojson", what = "sp")
## convert date to Date class
vStatsTeiRaw $date <- anydate(vStatsTeiRaw$date)
## filter out all rows for issues without marked up page breaks
vStatsTei <- filter(vStatsTeiRaw, number.of.pages >=10)
## add new column of words per page
vStatsTei$words.per.page <- vStatsTei$number.of.words / vStatsTei$number.of.pages
# 2. plot
## simple  map; use geom_polygon in ggplot2 to map dataframes
mapBase <- ggplot() +
geom_polygon(data = map_data("world"), aes(x=long, y = lat, group = group),
fill = NA, color = "gray50") +
coord_fixed(1.3)+ # fixes the relationship/aspect ratio between coordinates
guides(fill=FALSE)  # do this to leave off the color legend
mapBase
## zoom in
map2 <- mapBase +
coord_fixed(xlim = c(-123, -121.0),  ylim = c(36, 38), ratio = 1.3)
map2
## add locations from csv
map3 <- mapBase +
geom_point(data = vLocsBylines, aes(x = lng, y= lat), color = "gray",
#size = (vLocsBylines$number.of.bylines / 5)) # relative size
size = 5)
map3
## add locations from csv
map3 <- mapBase +
geom_point(data = vLocsBylines, aes(x = lng, y= lat, colour = number.of.bylines),
#size = (vLocsBylines$number.of.bylines / 5)) # relative size
size = 5)
map3
geom_point(data = vLocsBylines, aes(x = lng, y= lat, colour = number.of.bylines, size = number.of.bylines)
#size = (vLocsBylines$number.of.bylines / 5)) # relative size
# size = 5)
map3
## add locations from csv
map3 <- mapBase +
geom_point(data = vLocsBylines, aes(x = lng, y= lat, colour = number.of.bylines, size = number.of.bylines))
#size = (vLocsBylines$number.of.bylines / 5)) # relative size
#size = 5)
map3
## add locations from csv
map3 <- mapBase +
geom_point(data = vLocsBylines, aes(x = lng, y= lat, colour = number.of.bylines, size = number.of.bylines * 5))
#size = (vLocsBylines$number.of.bylines / 5)) # relative size
#size = 5)
map3
## plot geojson with ggplot requires data frames
map4 <- mapBase +
labs(title="المقتبس",
subtitle="Locations mentioned in bylines",
x="",
y="",
caption = "Till Grallert, CC BY-SA 4.0")+ # provides title, subtitle, x, y, caption
# layer: location data as points
geom_point(data = vLocsBylinesJsonDf, aes(x = long,y = lat, size = publications, color = publications),
alpha = 0.6)+
scale_size_continuous(range = c(1, 10)) # set the scale for the size of the location dots
# layer: add labels to locations. Problem: Arabic is incorrectly rendered on Mac
#geom_text(data = vLocsBylinesJsonDf,aes(x = long, y = lat, label = name, hjust = 0, vjust = 0.5, family = "Amiri"), check_overlap = T) # add labels
map4
## change the legend only: hint use `guides()`
map4 + guides(size = guide_legend(override.aes = list(size=3)))
# wordclouds
# Load libraries
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
library("arabicStemR")
# enable unicode
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenArabicPE/") #Volumes/Dessau HD/
# import text
text <- readLines(file.choose())
# read authorship stats from CSV
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# load text of Muqtabas 1/1
#v.Text.Muqtabas.1.1 <- readLines("digital-muqtabas/md/oclc_4770057679-i_1.md")
v.Text.Muqtabas.1.1<-scan("digital-muqtabas/md/oclc_4770057679-i_1.md", what="character", encoding="UTF-8", sep="\n")
# remove the YAML block
v.Text.Muqtabas.1.1 <- v.Text.Muqtabas.1.1[6:650]
# paste all lines together
v.Text.Muqtabas.1.1 <- paste(v.Text.Muqtabas.1.1, collapse = " ")
# load list of Arabic stopwords
v.Text.Stoplist.Arabic <- readLines("/BachUni/programming/R/RTextAnalysis/stoplist_arabic.txt") # https://github.com/mohataher/arabic-stop-words/blob/master/list.txt , https://raw.githubusercontent.com/6/stopwords-json/master/dist/ar.json
# Remove Arabic common stopwords
v.Text.Muqtabas.1.1.stop <- removeStopWords(v.Text.Muqtabas.1.1, defaultStopwordList = T, customStopwordList = v.Text.Stoplist.Arabic)
# wordclouds
# Load libraries
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
library("arabicStemR")
# enable unicode
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenArabicPE/") #Volumes/Dessau HD/
# import text
text <- readLines(file.choose())
# read authorship stats from CSV
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# load text of Muqtabas 1/1
#v.Text.Muqtabas.1.1 <- readLines("digital-muqtabas/md/oclc_4770057679-i_1.md")
v.Text.Muqtabas.1.1<-scan("digital-muqtabas/md/oclc_4770057679-i_1.md", what="character", encoding="UTF-8", sep="\n")
# remove the YAML block
v.Text.Muqtabas.1.1 <- v.Text.Muqtabas.1.1[6:650]
# paste all lines together
v.Text.Muqtabas.1.1 <- paste(v.Text.Muqtabas.1.1, collapse = " ")
# load list of Arabic stopwords
v.Text.Stoplist.Arabic <- readLines("/BachUni/programming/R/RTextAnalysis/stoplist_arabic.txt") # https://github.com/mohataher/arabic-stop-words/blob/master/list.txt , https://raw.githubusercontent.com/6/stopwords-json/master/dist/ar.json
# Remove Arabic common stopwords
v.Text.Muqtabas.1.1.stop <- removeStopWords(v.Text.Muqtabas.1.1, defaultStopwordList = T, customStopwordList = v.Text.Stoplist.Arabic)
# load data as corpus
docs <- Corpus(VectorSource(v.Text.Muqtabas.1.1.stop))
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
library("arabicStemR")
install.packages("arabicStemR", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
library("arabicStemR")
# load data as corpus
docs <- Corpus(VectorSource(v.Text.Muqtabas.1.1.stop))
install.packages(c("tm", "SnowballC", "wordcloud"), lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
library("arabicStemR")
library(dplyr) # for data cleaning
library(plyr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(arabicStemR)
# load data as corpus
docs <- Corpus(VectorSource(v.Text.Muqtabas.1.1.stop))
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# load text of Muqtabas 1/1
#v.Text.Muqtabas.1.1 <- readLines("digital-muqtabas/md/oclc_4770057679-i_1.md")
v.Text.Muqtabas.1.1<-scan("digital-muqtabas/md/oclc_4770057679-i_1.md", what="character", encoding="UTF-8", sep="\n")
# remove the YAML block
v.Text.Muqtabas.1.1 <- v.Text.Muqtabas.1.1[6:650]
# paste all lines together
v.Text.Muqtabas.1.1 <- paste(v.Text.Muqtabas.1.1, collapse = " ")
# load list of Arabic stopwords
v.Text.Stoplist.Arabic <- readLines("/BachUni/programming/R/RTextAnalysis/stoplist_arabic.txt") # https://github.com/mohataher/arabic-stop-words/blob/master/list.txt , https://raw.githubusercontent.com/6/stopwords-json/master/dist/ar.json
# Remove Arabic common stopwords
v.Text.Muqtabas.1.1.stop <- removeStopWords(v.Text.Muqtabas.1.1, defaultStopwordList = T, customStopwordList = v.Text.Stoplist.Arabic)
# load data as corpus
docs <- Corpus(VectorSource(v.Text.Muqtabas.1.1.stop))
View(docs)
# build a term-document matrix to get a frequency list
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# show the first ten lines of the resulting data frame
head(d, 30)
# wordcloud of authors and articles
set.seed(1234)
wordcloud(words = transliterate(v.Articles.Authors$name), # problem: R for macOS does not plot Arabic, solution: transliterate(v.Articles.Authors$name)
freq = v.Articles.Authors$articles,
min.freq = 1,
max.words=200,
random.order=F,
rot.per=0.35,
colors=brewer.pal(8, "Accent"))
View(d)
v.Freq.Muqtabas.1.1 <- data.frame(word = names(v),freq=v)
# wordclour of issue
wordcloud(words = transliterate(v.Freq.Muqtabas.1.1$word), # problem: R for macOS does not plot Arabic, solution: transliterate(v.Articles.Authors$name)
freq = v.Freq.Muqtabas.1.1$freq,
min.freq = 1,
max.words=200,
random.order=F,
rot.per=0.35,
colors=brewer.pal(8, "Accent"))
# wordclour of issue
wordcloud(words = v.Freq.Muqtabas.1.1$word, # problem: R for macOS does not plot Arabic, solution: transliterate(v.Freq.Muqtabas.1.1$word)
freq = v.Freq.Muqtabas.1.1$freq,
min.freq = 1,
max.words=200,
random.order=F,
rot.per=0.35,
colors=brewer.pal(8, "Accent"))
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenArabicPE/digital-mawaqif") #Volumes/Dessau HD/
# read stats from JSON
v.Articles.Authors.Json <- geojson_read("oclc_644997575-stats_mods.json", what = "sp")
install.packages("jsonlite", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# read json
library(jsonlite)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON("oclc_644997575-stats_mods.json")
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(oclc_644997575-stats_mods.json)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = oclc_644997575-stats_mods.json)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
install.packages("tidyjson", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("RJSONIO", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# read json
library(RJSONIO)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
library(dplyr) # for data cleaning
library(plyr)
# read json
library(RJSONIO)
#library(jsonlite)
# text mining, wordlcouds
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(arabicStemR)
# enable unicode
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenArabicPE/digital-mawaqif") #Volumes/Dessau HD/
# import text
# read authorship stats from CSV
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenArabicPE/digital-mawaqif/") #Volumes/Dessau HD/
# import text
# read authorship stats from CSV
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
detach("package:jsonlite", unload=TRUE)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
# read json
library(RJSONIO)
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
library(dplyr) # for data cleaning
library(plyr)
# read json
library(RJSONIO)
#library(jsonlite)
# text mining, wordlcouds
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(arabicStemR)
# enable unicode
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenAra
library(dplyr) # for data cleaning
library(plyr)
# read json
library(RJSONIO)
#library(jsonlite)
# text mining, wordlcouds
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(arabicStemR)
# enable unicode
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# use a working directory
setwd("/BachUni/BachBibliothek/GitHub/OpenAra
# import text
# read authorship stats from CSV
v.Articles.Authors <- read.csv("analytical-playground/metadata/statistics/oclc_4770057679-stats_mods.csv", header=TRUE, sep = ",", quote = "")
# read stats from JSON
v.Articles.Authors.Json <- fromJSON(txt = "oclc_644997575-stats_mods.json")
